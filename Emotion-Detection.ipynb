{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7881bbc",
   "metadata": {},
   "source": [
    "# **Learn Computer Vision**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7811c85d",
   "metadata": {},
   "source": [
    "**Learning Project For Computer Vision**\n",
    "\n",
    "Making a emotion detection using a CNN and Yolo\n",
    "\n",
    "**This Computer Vision project is using a Micro-Expression Dataset**\n",
    "\n",
    "[Micro-Expression Dataset (Kaggle Dataset)](https://www.kaggle.com/datasets/kmirfan/micro-expressions/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7d3a1c",
   "metadata": {},
   "source": [
    "## **Table of Content**\n",
    "1. Setup\n",
    "\n",
    "2. Dataset Prep\n",
    "\n",
    "3. EDA and Pre-Prosessing\n",
    "\n",
    "4. CNN Model Devlop\n",
    "\n",
    "5. YOLO Integration\n",
    "\n",
    "6. Model training and evaluation\n",
    "\n",
    "7. Real-time Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b91c881",
   "metadata": {},
   "source": [
    "### **1. Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1edd437",
   "metadata": {},
   "source": [
    "#### 1.1. Install Req Library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c50dbcf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n%pip install opencv-python tensorflow keras numpy pandas matplotlib scikit-learn\\n%pip install yolov5 torch torchvision\\n%pip install pillow seaborn\\npip install \"tensorflow<2.11\"\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use if needed to install packages\n",
    "\"\"\"\n",
    "%pip install opencv-python tensorflow keras numpy pandas matplotlib scikit-learn\n",
    "%pip install yolov5 torch torchvision\n",
    "%pip install pillow seaborn\n",
    "pip install \"tensorflow<2.11\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cac7e7",
   "metadata": {},
   "source": [
    "#### 1.2. Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dbb22a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs Available:  1\n",
      "GPUs name :  device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:2b:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import InceptionV3, MobileNetV2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For YOLO face detection\n",
    "import torch\n",
    "import yolov5\n",
    "\n",
    "# Check for GPU availability\n",
    "from tensorflow.python. client import device_lib\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"GPUs Available: \", len(gpus))\n",
    "    print(\"GPUs name : \", device_lib.list_local_devices()[-1].physical_device_desc)\n",
    "else:\n",
    "    print(\"No GPU Available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44203c2b",
   "metadata": {},
   "source": [
    "### **2. Dataset Prep**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5d97fd",
   "metadata": {},
   "source": [
    "#### 2.1. Set Dataset Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8bc5401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset structure:\n",
      "['test', 'train']\n"
     ]
    }
   ],
   "source": [
    "# Extract the dataset\n",
    "dataset_path = 'Micro_Expressions'\n",
    "\n",
    "# Check dataset structure\n",
    "print(\"\\nDataset structure:\")\n",
    "print(os.listdir(dataset_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7477a1",
   "metadata": {},
   "source": [
    "#### 2.2. Explore Dataset Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00880ab0",
   "metadata": {},
   "source": [
    "because there already 2 folder in dataset so it will be sparate to train_dir and test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7370b866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying emotion classes in train folder:\n",
      "Found emotions: ['anger', 'disgust', 'fear', 'happiness', 'neutral', 'sadness', 'surprise']\n",
      "\n",
      "Verifying emotion classes in test folder:\n",
      "Found emotions: ['anger', 'disgust', 'fear', 'happiness', 'neutral', 'sadness', 'surprise']\n",
      "Images per emotion in TRAIN set:\n",
      "  anger       : 1411 images\n",
      "  disgust     :  662 images\n",
      "  fear        :  479 images\n",
      "  happiness   : 1950 images\n",
      "  neutral     :  644 images\n",
      "  sadness     : 1369 images\n",
      "  surprise    : 1085 images\n",
      "Images per emotion in TEST set:\n",
      "  anger       :  350 images\n",
      "  disgust     :  160 images\n",
      "  fear        :  120 images\n",
      "  happiness   :  480 images\n",
      "  neutral     :  160 images\n",
      "  sadness     :  330 images\n",
      "  surprise    :  260 images\n",
      "Total train images: 7600\n",
      "Total test images:  1860\n",
      "Total images:       9460\n"
     ]
    }
   ],
   "source": [
    "# Define emotion classes (7 emotions in the dataset)\n",
    "EMOTION_CLASSES = ['anger', 'disgust', 'fear', 'happiness', 'neutral', 'sadness', 'surprise']\n",
    "\n",
    "# Define paths for train and test folders\n",
    "train_dir = os.path.join(dataset_path, 'train')\n",
    "test_dir = os.path.join(dataset_path, 'test')\n",
    "\n",
    "# Verify all emotion classes exist\n",
    "print(\"Verifying emotion classes in train folder:\")\n",
    "train_emotions = os.listdir(train_dir)\n",
    "print(f\"Found emotions: {sorted(train_emotions)}\")\n",
    "\n",
    "print(\"\\nVerifying emotion classes in test folder:\")\n",
    "test_emotions = os.listdir(test_dir)\n",
    "print(f\"Found emotions: {sorted(test_emotions)}\")\n",
    "\n",
    "# Count images per emotion in train set\n",
    "train_emotion_counts = {}\n",
    "for emotion in EMOTION_CLASSES:\n",
    "    emotion_path = os.path.join(train_dir, emotion)\n",
    "    if os.path.isdir(emotion_path):\n",
    "        train_emotion_counts[emotion] = len(os.listdir(emotion_path))\n",
    "\n",
    "# Count images per emotion in test set\n",
    "test_emotion_counts = {}\n",
    "for emotion in EMOTION_CLASSES:\n",
    "    emotion_path = os.path.join(test_dir, emotion)\n",
    "    if os.path.isdir(emotion_path):\n",
    "        test_emotion_counts[emotion] = len(os.listdir(emotion_path))\n",
    "\n",
    "# Print counts\n",
    "print(\"Images per emotion in TRAIN set:\")\n",
    "total_train = 0\n",
    "for emotion in EMOTION_CLASSES:\n",
    "    count = train_emotion_counts.get(emotion, 0)\n",
    "    print(f\"  {emotion:12s}: {count:4d} images\")\n",
    "    total_train += count\n",
    "\n",
    "print(\"Images per emotion in TEST set:\")\n",
    "total_test = 0\n",
    "for emotion in EMOTION_CLASSES:\n",
    "    count = test_emotion_counts.get(emotion, 0)\n",
    "    print(f\"  {emotion:12s}: {count:4d} images\")\n",
    "    total_test += count\n",
    "\n",
    "print(f\"Total train images: {total_train}\")\n",
    "print(f\"Total test images:  {total_test}\")\n",
    "print(f\"Total images:       {total_train + total_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf123ff5",
   "metadata": {},
   "source": [
    "#### 2.3. Visualize Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ef42056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample image shape: (80, 80, 3)\n",
      "Sample image dtype: uint8\n"
     ]
    }
   ],
   "source": [
    "# Display sample images from each emotion (from train set)\n",
    "num_emotions = len(EMOTION_CLASSES)\n",
    "num_cols = 4\n",
    "num_rows = (num_emotions + num_cols - 1) // num_cols\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, emotion in enumerate(EMOTION_CLASSES):\n",
    "    emotion_path = os.path.join(train_dir, emotion)\n",
    "    image_files = os.listdir(emotion_path)\n",
    "    \n",
    "    if len(image_files) > 0:\n",
    "        sample_image = image_files [0]\n",
    "        img_path = os.path.join(emotion_path, sample_image)\n",
    "        \n",
    "        img = cv2.imread(img_path)\n",
    "        if img is not None:\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            axes[idx].imshow(img_rgb)\n",
    "            axes[idx].set_title(f'{emotion.capitalize()}', fontsize=12, fontweight='bold')\n",
    "            axes[idx].axis('off')\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(len(EMOTION_CLASSES), len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Sample image shape: {img_rgb.shape}\")\n",
    "print(f\"Sample image dtype: {img_rgb.dtype}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689f1b0e",
   "metadata": {},
   "source": [
    "### **3. Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b2a192",
   "metadata": {},
   "source": [
    "#### 3.1. Load and Preprocess Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eba4f759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN images\n",
      "Loaded 7600/7600 images...\n",
      "Loading complete! 7600 images loaded\n",
      "Loaded 7600 train images\n",
      "\n",
      "TEST images\n",
      "Loaded 1800/1860 images...\n",
      "Loading complete! 1860 images loaded\n",
      "Loaded 1860 test images\n",
      "\n",
      "Image shape: (7600, 80, 80, 3)\n",
      "Unique emotions in train set: ['anger' 'disgust' 'fear' 'happiness' 'neutral' 'sadness' 'surprise']\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = 80  # Micro-expressions dataset uses 80x80 images\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def load_images_and_labels(data_dir, emotion_classes, img_size=IMG_SIZE):\n",
    "    \"\"\"Load all images and labels from a directory (train or test)\"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # Calculate total files for progress tracking\n",
    "    total_files = 0\n",
    "    for emotion in emotion_classes:\n",
    "        emotion_path = os.path.join(data_dir, emotion)\n",
    "        if os.path.isdir(emotion_path):\n",
    "            total_files += len(os.listdir(emotion_path))\n",
    "    \n",
    "    loaded = 0\n",
    "    \n",
    "    for emotion in emotion_classes:\n",
    "        emotion_path = os.path.join(data_dir, emotion)\n",
    "        if not os.path.isdir(emotion_path):\n",
    "            print(f\"Warning: {emotion_path} not found\")\n",
    "            continue\n",
    "            \n",
    "        for img_name in os.listdir(emotion_path):\n",
    "            try:\n",
    "                img_path = os.path.join(emotion_path, img_name)\n",
    "                img = cv2.imread(img_path)\n",
    "                \n",
    "                if img is not None:\n",
    "                    # Resize image to standard size\n",
    "                    img_resized = cv2.resize(img, (img_size, img_size))\n",
    "                    \n",
    "                    # Normalize pixel values to 0-1 range\n",
    "                    img_normalized = img_resized / 255.0\n",
    "                    \n",
    "                    images.append(img_normalized)\n",
    "                    labels.append(emotion)\n",
    "                    loaded += 1\n",
    "                    \n",
    "                    if loaded % 100 == 0:\n",
    "                        print(f\"Loaded {loaded}/{total_files} images...\", end='\\r')\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {img_path}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    print(f\"\\nLoading complete! {loaded} images loaded\")\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Load train and test datasets\n",
    "print(\"TRAIN images\")\n",
    "X_train_full, y_train_full = load_images_and_labels(train_dir, EMOTION_CLASSES)\n",
    "print(f\"Loaded {len(X_train_full)} train images\")\n",
    "\n",
    "print(\"\\nTEST images\")\n",
    "X_test_full, y_test_full = load_images_and_labels(test_dir, EMOTION_CLASSES)\n",
    "print(f\"Loaded {len(X_test_full)} test images\")\n",
    "\n",
    "print(f\"\\nImage shape: {X_train_full.shape}\")\n",
    "print(f\"Unique emotions in train set: {np.unique(y_train_full)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f9bb1f",
   "metadata": {},
   "source": [
    "#### 3.2. Encode Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "815e3743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion mapping:\n",
      "  0: anger\n",
      "  1: disgust\n",
      "  2: fear\n",
      "  3: happiness\n",
      "  4: neutral\n",
      "  5: sadness\n",
      "  6: surprise\n"
     ]
    }
   ],
   "source": [
    "# Encode emotion labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train_full)\n",
    "y_test_encoded = label_encoder.transform(y_test_full)\n",
    "\n",
    "# Create a dictionary for emotion mapping\n",
    "emotion_map = {i: emotion for i, emotion in enumerate(label_encoder.classes_)}\n",
    "reverse_emotion_map = {emotion: i for i, emotion in emotion_map.items()}\n",
    "\n",
    "print(\"Emotion mapping:\")\n",
    "for code, emotion in sorted(emotion_map.items()):\n",
    "    print(f\"  {code}: {emotion}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f362c3",
   "metadata": {},
   "source": [
    "#### 3.3. Split Training Dataset into Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1581e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split summary:\n",
      "Training set:   6080 images (80% of train folder)\n",
      "Validation set: 1520 images (20% of train folder)\n",
      "Test set:       1860 images (all test folder)\n"
     ]
    }
   ],
   "source": [
    "# Further split the training set into training and validation sets (80-20 split)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_encoded, test_size=0.2, random_state=42, stratify=y_train_encoded\n",
    ")\n",
    "\n",
    "# Use the original test set as our final test set\n",
    "X_test = X_test_full\n",
    "y_test = y_test_encoded\n",
    "\n",
    "print(\"Data split summary:\")\n",
    "print(f\"Training set:   {len(X_train)} images (80% of train folder)\")\n",
    "print(f\"Validation set: {len(X_val)} images (20% of train folder)\")\n",
    "print(f\"Test set:       {len(X_test)} images (all test folder)\")\n",
    "\n",
    "# Convert labels to categorical format for neural network\n",
    "y_train_cat = keras.utils.to_categorical(y_train, num_classes=len(emotion_map))\n",
    "y_val_cat = keras.utils.to_categorical(y_val, num_classes=len(emotion_map))\n",
    "y_test_cat = keras.utils.to_categorical(y_test, num_classes=len(emotion_map))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4b83bd",
   "metadata": {},
   "source": [
    "#### 3.4. Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c46b6805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data augmentation pipeline for training set\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    zoom_range=0.2,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Fit augmentation on training data\n",
    "train_datagen.fit(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9a8326",
   "metadata": {},
   "source": [
    "### **4. CNN Model Development**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b327f865",
   "metadata": {},
   "source": [
    "#### 4.1. Build Custom CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8e80257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Model Architecture:\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 80, 80, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 80, 80, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 80, 80, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 80, 80, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 40, 40, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 40, 40, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 40, 40, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 40, 40, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 40, 40, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 40, 40, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 20, 20, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 20, 20, 64)        0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 20, 20, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 20, 20, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 20, 20, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 20, 20, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 10, 10, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 10, 10, 128)       0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 12800)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               6554112   \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 7)                 1799      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,979,111\n",
      "Trainable params: 6,976,679\n",
      "Non-trainable params: 2,432\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def create_cnn_model(input_shape, num_classes):\n",
    "    \"\"\"Create a custom CNN model for emotion detection\"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Block 1 - 32 filters\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Block 2 - 64 filters\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Block 3 - 128 filters\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Flatten and Dense layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "input_shape = (IMG_SIZE, IMG_SIZE, 3)\n",
    "num_classes = len(emotion_map)\n",
    "model = create_cnn_model(input_shape, num_classes)\n",
    "\n",
    "print(\"CNN Model Architecture:\")\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e5b048",
   "metadata": {},
   "source": [
    "#### 4.2. Alternative: Transfer Learning with MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24afd2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For faster training on limited resources\n",
    "def create_transfer_learning_model(input_shape, num_classes):\n",
    "    \"\"\"Create model using transfer learning with MobileNetV2\"\"\"\n",
    "    base_model = MobileNetV2(\n",
    "        input_shape=input_shape,\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    \n",
    "    # Freeze base model layers\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "# Uncomment to use transfer learning instead of custom CNN\n",
    "# model, base_model = create_transfer_learning_model(input_shape, num_classes)\n",
    "# print(\"Transfer Learning Model created with MobileNetV2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aa3625",
   "metadata": {},
   "source": [
    "#### 4.3. Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85940853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82daa28e",
   "metadata": {},
   "source": [
    "### **5. Model Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c451a379",
   "metadata": {},
   "source": [
    "#### 5.1. Define Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8876badd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping callback - stops training when validation loss stops improving\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=20,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Learning rate reduction callback - reduces learning rate when progress plateaus\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping, reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6ff2f4",
   "metadata": {},
   "source": [
    "#### 5.2. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c25942d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "190/190 [==============================] - 17s 58ms/step - loss: 2.8656 - accuracy: 0.1531 - val_loss: 1.9855 - val_accuracy: 0.0914 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "190/190 [==============================] - 10s 53ms/step - loss: 2.7266 - accuracy: 0.1484 - val_loss: 1.9945 - val_accuracy: 0.1776 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "190/190 [==============================] - 10s 55ms/step - loss: 2.5999 - accuracy: 0.1681 - val_loss: 1.9739 - val_accuracy: 0.2217 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "190/190 [==============================] - 11s 58ms/step - loss: 2.5091 - accuracy: 0.1755 - val_loss: 2.0124 - val_accuracy: 0.1816 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "190/190 [==============================] - 11s 58ms/step - loss: 2.4847 - accuracy: 0.1765 - val_loss: 2.3697 - val_accuracy: 0.1855 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "190/190 [==============================] - 12s 62ms/step - loss: 2.3966 - accuracy: 0.1906 - val_loss: 2.4485 - val_accuracy: 0.1914 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "190/190 [==============================] - 12s 64ms/step - loss: 2.3522 - accuracy: 0.1931 - val_loss: 2.6270 - val_accuracy: 0.1849 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 2.3586 - accuracy: 0.1959\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "190/190 [==============================] - 12s 62ms/step - loss: 2.3586 - accuracy: 0.1959 - val_loss: 2.4071 - val_accuracy: 0.1539 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "190/190 [==============================] - 12s 62ms/step - loss: 2.3227 - accuracy: 0.1969 - val_loss: 2.1670 - val_accuracy: 0.1539 - lr: 5.0000e-05\n",
      "Epoch 10/100\n",
      "190/190 [==============================] - 12s 61ms/step - loss: 2.3343 - accuracy: 0.2054 - val_loss: 2.1151 - val_accuracy: 0.1980 - lr: 5.0000e-05\n",
      "Epoch 11/100\n",
      "190/190 [==============================] - 12s 61ms/step - loss: 2.3024 - accuracy: 0.2138 - val_loss: 2.3887 - val_accuracy: 0.1868 - lr: 5.0000e-05\n",
      "Epoch 12/100\n",
      "190/190 [==============================] - 11s 60ms/step - loss: 2.2822 - accuracy: 0.2168 - val_loss: 2.0895 - val_accuracy: 0.2000 - lr: 5.0000e-05\n",
      "Epoch 13/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 2.2756 - accuracy: 0.2084\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "190/190 [==============================] - 11s 60ms/step - loss: 2.2756 - accuracy: 0.2084 - val_loss: 2.2161 - val_accuracy: 0.1941 - lr: 5.0000e-05\n",
      "Epoch 14/100\n",
      "190/190 [==============================] - 12s 63ms/step - loss: 2.2681 - accuracy: 0.2095 - val_loss: 2.0909 - val_accuracy: 0.1961 - lr: 2.5000e-05\n",
      "Epoch 15/100\n",
      "190/190 [==============================] - 11s 58ms/step - loss: 2.2233 - accuracy: 0.2086 - val_loss: 2.0495 - val_accuracy: 0.2118 - lr: 2.5000e-05\n",
      "Epoch 16/100\n",
      "190/190 [==============================] - 12s 61ms/step - loss: 2.2683 - accuracy: 0.2115 - val_loss: 1.9622 - val_accuracy: 0.2461 - lr: 2.5000e-05\n",
      "Epoch 17/100\n",
      "190/190 [==============================] - 12s 64ms/step - loss: 2.2717 - accuracy: 0.2087 - val_loss: 1.9808 - val_accuracy: 0.2066 - lr: 2.5000e-05\n",
      "Epoch 18/100\n",
      "190/190 [==============================] - 14s 75ms/step - loss: 2.2287 - accuracy: 0.2151 - val_loss: 1.9698 - val_accuracy: 0.2053 - lr: 2.5000e-05\n",
      "Epoch 19/100\n",
      "190/190 [==============================] - 14s 72ms/step - loss: 2.2484 - accuracy: 0.2128 - val_loss: 2.0674 - val_accuracy: 0.1987 - lr: 2.5000e-05\n",
      "Epoch 20/100\n",
      "190/190 [==============================] - 14s 71ms/step - loss: 2.2403 - accuracy: 0.2192 - val_loss: 2.2071 - val_accuracy: 0.1980 - lr: 2.5000e-05\n",
      "Epoch 21/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 2.2389 - accuracy: 0.2161\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "190/190 [==============================] - 13s 70ms/step - loss: 2.2389 - accuracy: 0.2161 - val_loss: 2.1243 - val_accuracy: 0.1882 - lr: 2.5000e-05\n",
      "Epoch 22/100\n",
      "190/190 [==============================] - 14s 72ms/step - loss: 2.2138 - accuracy: 0.2171 - val_loss: 2.1353 - val_accuracy: 0.1914 - lr: 1.2500e-05\n",
      "Epoch 23/100\n",
      "190/190 [==============================] - 13s 69ms/step - loss: 2.2132 - accuracy: 0.2104 - val_loss: 2.0959 - val_accuracy: 0.2007 - lr: 1.2500e-05\n",
      "Epoch 24/100\n",
      "190/190 [==============================] - 12s 62ms/step - loss: 2.2301 - accuracy: 0.2146 - val_loss: 2.1163 - val_accuracy: 0.1914 - lr: 1.2500e-05\n",
      "Epoch 25/100\n",
      "190/190 [==============================] - 13s 68ms/step - loss: 2.1987 - accuracy: 0.2183 - val_loss: 2.1691 - val_accuracy: 0.1822 - lr: 1.2500e-05\n",
      "Epoch 26/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 2.2315 - accuracy: 0.2128\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "190/190 [==============================] - 12s 63ms/step - loss: 2.2315 - accuracy: 0.2128 - val_loss: 2.0778 - val_accuracy: 0.2105 - lr: 1.2500e-05\n",
      "Epoch 27/100\n",
      "190/190 [==============================] - 12s 61ms/step - loss: 2.2106 - accuracy: 0.2146 - val_loss: 2.1029 - val_accuracy: 0.2158 - lr: 6.2500e-06\n",
      "Epoch 28/100\n",
      "190/190 [==============================] - 13s 67ms/step - loss: 2.2030 - accuracy: 0.2171 - val_loss: 2.1045 - val_accuracy: 0.1974 - lr: 6.2500e-06\n",
      "Epoch 29/100\n",
      "190/190 [==============================] - 12s 61ms/step - loss: 2.2116 - accuracy: 0.2137 - val_loss: 2.1624 - val_accuracy: 0.1921 - lr: 6.2500e-06\n",
      "Epoch 30/100\n",
      "190/190 [==============================] - 12s 63ms/step - loss: 2.2188 - accuracy: 0.2178 - val_loss: 2.1321 - val_accuracy: 0.1908 - lr: 6.2500e-06\n",
      "Epoch 31/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 2.2321 - accuracy: 0.2094\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "190/190 [==============================] - 12s 62ms/step - loss: 2.2321 - accuracy: 0.2094 - val_loss: 2.0853 - val_accuracy: 0.2066 - lr: 6.2500e-06\n",
      "Epoch 32/100\n",
      "190/190 [==============================] - 11s 60ms/step - loss: 2.1925 - accuracy: 0.2178 - val_loss: 2.1243 - val_accuracy: 0.1908 - lr: 3.1250e-06\n",
      "Epoch 33/100\n",
      "190/190 [==============================] - 11s 59ms/step - loss: 2.2150 - accuracy: 0.2122 - val_loss: 2.1467 - val_accuracy: 0.1914 - lr: 3.1250e-06\n",
      "Epoch 34/100\n",
      "190/190 [==============================] - 12s 62ms/step - loss: 2.2064 - accuracy: 0.2148 - val_loss: 2.1581 - val_accuracy: 0.1934 - lr: 3.1250e-06\n",
      "Epoch 35/100\n",
      "190/190 [==============================] - 12s 61ms/step - loss: 2.1912 - accuracy: 0.2179 - val_loss: 2.1622 - val_accuracy: 0.1829 - lr: 3.1250e-06\n",
      "Epoch 36/100\n",
      "190/190 [==============================] - ETA: 0s - loss: 2.2213 - accuracy: 0.2062Restoring model weights from the end of the best epoch: 16.\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "190/190 [==============================] - 11s 60ms/step - loss: 2.2213 - accuracy: 0.2062 - val_loss: 2.1481 - val_accuracy: 0.1862 - lr: 3.1250e-06\n",
      "Epoch 36: early stopping\n",
      "Training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "EPOCHS = 100\n",
    "\n",
    "history = model.fit(\n",
    "    train_datagen.flow(X_train, y_train_cat, batch_size=BATCH_SIZE),\n",
    "    validation_data=(X_val, y_val_cat),\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Training completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed07ec9d",
   "metadata": {},
   "source": [
    "#### 5.3. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09e35100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total epochs trained: 36\n",
      "Final training accuracy: 0.2062\n",
      "Final validation accuracy: 0.1862\n"
     ]
    }
   ],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "axes[0].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title('Model Accuracy Over Epochs', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss plot\n",
    "axes[1].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[1].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Loss', fontsize=12)\n",
    "axes[1].set_title('Model Loss Over Epochs', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTotal epochs trained: {len(history.history['loss'])}\")\n",
    "print(f\"Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a24829",
   "metadata": {},
   "source": [
    "### **6. Model Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6593d3",
   "metadata": {},
   "source": [
    "#### 6.1. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed7a057a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST SET EVALUATION RESULTS\n",
      "Test Loss:     1.9844\n",
      "Test Accuracy: 0.2409\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger     0.2065    0.3429    0.2578       350\n",
      "     disgust     0.0000    0.0000    0.0000       160\n",
      "        fear     0.0000    0.0000    0.0000       120\n",
      "   happiness     0.4048    0.2792    0.3305       480\n",
      "     neutral     0.0000    0.0000    0.0000       160\n",
      "     sadness     0.0000    0.0000    0.0000       330\n",
      "    surprise     0.2046    0.7462    0.3212       260\n",
      "\n",
      "    accuracy                         0.2409      1860\n",
      "   macro avg     0.1166    0.1955    0.1299      1860\n",
      "weighted avg     0.1719    0.2409    0.1787      1860\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "\n",
    "print(\"TEST SET EVALUATION RESULTS\")\n",
    "print(f\"Test Loss:     {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# Get predictions\n",
    "y_pred = model.predict(X_test, verbose=0)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(\n",
    "    y_test, \n",
    "    y_pred_classes, \n",
    "    target_names=[emotion_map[i] for i in range(len(emotion_map))],\n",
    "    digits=4\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388bcb35",
   "metadata": {},
   "source": [
    "#### 6.2. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f007c16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    cm, \n",
    "    annot=True, \n",
    "    fmt='d', \n",
    "    cmap='Blues',\n",
    "    xticklabels=[emotion_map[i] for i in range(len(emotion_map))],\n",
    "    yticklabels=[emotion_map[i] for i in range(len(emotion_map))],\n",
    "    cbar_kws={'label': 'Count'},\n",
    "    annot_kws={'size': 11}\n",
    ")\n",
    "plt.title('Confusion Matrix - Emotion Detection Model', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bf3cfb",
   "metadata": {},
   "source": [
    "#### 6.3. Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4c8e7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: ./models\\emotion_detection_model.h5\n",
      "Label mapping saved to: ./models\\emotion_label_encoder.pkl\n",
      "Model architecture saved to: ./models\\model_architecture.json\n",
      "Model weights saved to: ./models\\model_weights.h5\n",
      "\n",
      "All model files saved in './models' directory\n"
     ]
    }
   ],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "output_dir = './models'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save emotion detection model\n",
    "model_path = os.path.join(output_dir, 'emotion_detection_model.h5')\n",
    "model.save(model_path)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# Save label encoder mapping\n",
    "import pickle\n",
    "encoder_path = os.path.join(output_dir, 'emotion_label_encoder.pkl')\n",
    "with open(encoder_path, 'wb') as f:\n",
    "    pickle.dump(emotion_map, f)\n",
    "print(f\"Label mapping saved to: {encoder_path}\")\n",
    "\n",
    "# Save model architecture as JSON\n",
    "json_path = os.path.join(output_dir, 'model_architecture.json')\n",
    "model_json = model.to_json()\n",
    "with open(json_path, 'w') as f:\n",
    "    f.write(model_json)\n",
    "print(f\"Model architecture saved to: {json_path}\")\n",
    "\n",
    "# Save model weights\n",
    "weights_path = os.path.join(output_dir, 'model_weights.h5')\n",
    "model.save_weights(weights_path)\n",
    "print(f\"Model weights saved to: {weights_path}\")\n",
    "\n",
    "print(f\"\\nAll model files saved in '{output_dir}' directory\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6bda54",
   "metadata": {},
   "source": [
    "### **7. YOLO Integration for Face Detection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3c7b70",
   "metadata": {},
   "source": [
    "#### 7.1. Load YOLO for Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f72f4bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face detector (Haar Cascade) loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Use pre-trained face detector from OpenCV (simpler and faster)\n",
    "face_cascade = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    ")\n",
    "\n",
    "print(\"Face detector (Haar Cascade) loaded successfully\")\n",
    "\n",
    "# Alternative: Download YOLOv5 face detection model (uncomment if needed)\n",
    "# model_yolo = torch.hub.load('ultralytics/yolov5', 'custom', \n",
    "#                             path='path/to/yolov5_face_weights.pt')\n",
    "# print(\"YOLO face detector loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5831c5b",
   "metadata": {},
   "source": [
    "#### 7.2. Emotion Detection on Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25dc8b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 1 face(s)\n",
      "  Face 1: Happiness (confidence: 0.2819)\n"
     ]
    }
   ],
   "source": [
    "def detect_emotions_in_image(image_path, emotion_model, face_cascade, emotion_classes):\n",
    "    \"\"\"Detect emotions in an image using face detection + CNN\"\"\"\n",
    "    \n",
    "    # Read image\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Error: Cannot read image from {image_path}\")\n",
    "        return None, []\n",
    "    \n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect faces\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        # Extract face region\n",
    "        face_region = img_rgb[y:y+h, x:x+w]\n",
    "        \n",
    "        # Preprocess face for emotion model\n",
    "        face_resized = cv2.resize(face_region, (IMG_SIZE, IMG_SIZE))\n",
    "        face_normalized = face_resized / 255.0\n",
    "        face_input = np.expand_dims(face_normalized, axis=0)\n",
    "        \n",
    "        # Predict emotion\n",
    "        emotion_pred = emotion_model.predict(face_input, verbose=0)\n",
    "        emotion_idx = np.argmax(emotion_pred[0])\n",
    "        emotion = emotion_map[emotion_idx]\n",
    "        confidence = emotion_pred[0][emotion_idx]\n",
    "        \n",
    "        results.append({\n",
    "            'position': (x, y, w, h),\n",
    "            'emotion': emotion,\n",
    "            'confidence': confidence\n",
    "        })\n",
    "        \n",
    "        # Draw rectangle and label on image\n",
    "        cv2.rectangle(img_rgb, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        label = f\"{emotion} ({confidence:.2f})\"\n",
    "        cv2.putText(img_rgb, label, (x, y-10), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    \n",
    "    return img_rgb, results\n",
    "\n",
    "# Test on an image from the test set\n",
    "test_emotion = EMOTION_CLASSES[0]  # ✅ Select FIRST emotion from the list\n",
    "test_emotion_path = os.path.join(test_dir, test_emotion)\n",
    "test_image = os.listdir(test_emotion_path)[0]  # ✅ Also select FIRST image\n",
    "test_image_path = os.path.join(test_emotion_path, test_image)\n",
    "\n",
    "result_img, detections = detect_emotions_in_image(test_image_path, model, face_cascade, EMOTION_CLASSES)\n",
    "\n",
    "if result_img is not None:\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(result_img)\n",
    "    plt.title(\"Emotion Detection Results from Test Set\", fontsize=14, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Detected {len(detections)} face(s)\")\n",
    "    for i, detection in enumerate(detections):\n",
    "        print(f\"  Face {i+1}: {detection['emotion'].capitalize()} (confidence: {detection['confidence']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac146dfb",
   "metadata": {},
   "source": [
    "### **8. Real-Time Emotion Detection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49160847",
   "metadata": {},
   "source": [
    "#### 8.1. Real-Time Emotion Detection from Webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d26879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting real-time emotion detection...\n",
      "Press 'q' to exit\n",
      "Real-time detection stopped\n"
     ]
    }
   ],
   "source": [
    "def real_time_emotion_detection(emotion_model, face_cascade, emotion_classes):\n",
    "    \"\"\"Real-time emotion detection from webcam\"\"\"\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Cannot access webcam\")\n",
    "        return\n",
    "    \n",
    "    print(\"Starting real-time emotion detection...\")\n",
    "    print(\"Press 'q' to exit\")\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Cannot read from webcam\")\n",
    "            break\n",
    "        \n",
    "        # Flip frame horizontally for mirror effect\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Detect faces\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "        \n",
    "        for (x, y, w, h) in faces:\n",
    "            # Extract and preprocess face\n",
    "            face_region = frame[y:y+h, x:x+w]\n",
    "            face_resized = cv2.resize(face_region, (IMG_SIZE, IMG_SIZE))\n",
    "            face_normalized = face_resized / 255.0\n",
    "            face_input = np.expand_dims(face_normalized, axis=0)\n",
    "            \n",
    "            # Predict emotion\n",
    "            emotion_pred = emotion_model.predict(face_input, verbose=0)\n",
    "            emotion_idx = np.argmax(emotion_pred[0])  # ✅ Access [0] for batch\n",
    "            emotion = emotion_map[emotion_idx]\n",
    "            confidence = emotion_pred[0][emotion_idx]  # ✅ Access [0] for batch first\n",
    "            \n",
    "            # Draw rectangle\n",
    "            color = (0, 255, 0)\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "            \n",
    "            # Add label\n",
    "            label = f\"{emotion} ({confidence:.2f})\"\n",
    "            cv2.putText(frame, label, (x, y-10), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "        \n",
    "        # Display frame\n",
    "        cv2.imshow('Real-Time Emotion Detection (Press q to exit)', frame)\n",
    "        \n",
    "        # Press 'q' to exit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Real-time detection stopped\")\n",
    "\n",
    "# Uncomment to run real-time detection\n",
    "# real_time_emotion_detection(model, face_cascade, EMOTION_CLASSES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec07f7f2",
   "metadata": {},
   "source": [
    "### **9. Advanced YOLO Integration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e5f879",
   "metadata": {},
   "source": [
    "#### 9.1. Using YOLOv5 for Better Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ec529e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef detect_emotions_yolov5(image_path, emotion_model, yolo_model, emotion_classes):\\n     # Detect emotions using YOLOv5 for face detection\\n    \\n    # Read image\\n    img = cv2.imread(image_path)\\n    if img is None:\\n        print(f\"Error: Cannot read image from {image_path}\")\\n        return None, []\\n    \\n    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\\n    \\n    # YOLOv5 detection\\n    results = yolo_model(img_rgb)\\n    detections = results.xyxy[0].cpu().numpy()  # ✅ Access [0] for first batch\\n    \\n    emotion_results = []\\n    \\n    for detection in detections:\\n        x1, y1, x2, y2, conf, cls = detection\\n        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\\n        \\n        if conf > 0.5:  # Confidence threshold\\n            # Extract face region\\n            face_region = img_rgb[y1:y2, x1:x2]\\n            \\n            # Preprocess for emotion model\\n            face_resized = cv2.resize(face_region, (IMG_SIZE, IMG_SIZE))\\n            face_normalized = face_resized / 255.0\\n            face_input = np.expand_dims(face_normalized, axis=0)\\n            \\n            # Predict emotion\\n            emotion_pred = emotion_model.predict(face_input, verbose=0)\\n            emotion_idx = np.argmax(emotion_pred[0])  # ✅ Access [0] for batch\\n            emotion = emotion_map[emotion_idx]\\n            emotion_conf = emotion_pred[0][emotion_idx]  # ✅ Access [0] for batch first\\n            \\n            emotion_results.append({\\n                \\'bbox\\': (x1, y1, x2, y2),\\n                \\'emotion\\': emotion,\\n                \\'confidence\\': float(emotion_conf)  # ✅ Convert to float for JSON serialization\\n            })\\n            \\n            # Draw on image\\n            cv2.rectangle(img_rgb, (x1, y1), (x2, y2), (0, 255, 0), 2)\\n            label = f\"{emotion} ({emotion_conf:.2f})\"\\n            cv2.putText(img_rgb, label, (x1, y1-10), \\n                       cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\\n    \\n    return img_rgb, emotion_results\\n\\n\\n# To use YOLOv5, first load the model:\\n# First, download YOLOv5 face weights\\n# You can use a pre-trained face detection model like:\\n# !wget https://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5s.pt\\n\\n# Then load it:\\nmodel_yolo = torch.hub.load(\\'ultralytics/yolov5\\', \\'custom\\', path=\\'yolov5s.pt\\')\\n\\n# Or use a face-specific YOLOv5 model:\\n# model_yolo = torch.hub.load(\\'ultralytics/yolov5\\', \\'custom\\', \\n#                             path=\\'path/to/yolov5_face_weights.pt\\')\\n\\n# Then test:\\nresult_img, detections = detect_emotions_yolov5(test_image_path, model, model_yolo, EMOTION_CLASSES)\\nif result_img is not None:\\nplt.figure(figsize=(12, 10))\\nplt.imshow(result_img)\\nplt.title(\"Emotion Detection with YOLOv5\", fontsize=14, fontweight=\\'bold\\')\\nplt.axis(\\'off\\')\\nplt.show()\\nprint(f\"Detected {len(detections)} face(s)\")\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def detect_emotions_yolov5(image_path, emotion_model, yolo_model, emotion_classes):\n",
    "     # Detect emotions using YOLOv5 for face detection\n",
    "    \n",
    "    # Read image\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Error: Cannot read image from {image_path}\")\n",
    "        return None, []\n",
    "    \n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # YOLOv5 detection\n",
    "    results = yolo_model(img_rgb)\n",
    "    detections = results.xyxy[0].cpu().numpy()  # ✅ Access [0] for first batch\n",
    "    \n",
    "    emotion_results = []\n",
    "    \n",
    "    for detection in detections:\n",
    "        x1, y1, x2, y2, conf, cls = detection\n",
    "        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "        \n",
    "        if conf > 0.5:  # Confidence threshold\n",
    "            # Extract face region\n",
    "            face_region = img_rgb[y1:y2, x1:x2]\n",
    "            \n",
    "            # Preprocess for emotion model\n",
    "            face_resized = cv2.resize(face_region, (IMG_SIZE, IMG_SIZE))\n",
    "            face_normalized = face_resized / 255.0\n",
    "            face_input = np.expand_dims(face_normalized, axis=0)\n",
    "            \n",
    "            # Predict emotion\n",
    "            emotion_pred = emotion_model.predict(face_input, verbose=0)\n",
    "            emotion_idx = np.argmax(emotion_pred[0])  # ✅ Access [0] for batch\n",
    "            emotion = emotion_map[emotion_idx]\n",
    "            emotion_conf = emotion_pred[0][emotion_idx]  # ✅ Access [0] for batch first\n",
    "            \n",
    "            emotion_results.append({\n",
    "                'bbox': (x1, y1, x2, y2),\n",
    "                'emotion': emotion,\n",
    "                'confidence': float(emotion_conf)  # ✅ Convert to float for JSON serialization\n",
    "            })\n",
    "            \n",
    "            # Draw on image\n",
    "            cv2.rectangle(img_rgb, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            label = f\"{emotion} ({emotion_conf:.2f})\"\n",
    "            cv2.putText(img_rgb, label, (x1, y1-10), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    \n",
    "    return img_rgb, emotion_results\n",
    "\n",
    "\n",
    "# To use YOLOv5, first load the model:\n",
    "# First, download YOLOv5 face weights\n",
    "# You can use a pre-trained face detection model like:\n",
    "# !wget https://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5s.pt\n",
    "\n",
    "# Then load it:\n",
    "model_yolo = torch.hub.load('ultralytics/yolov5', 'custom', path='yolov5s.pt')\n",
    "\n",
    "# Or use a face-specific YOLOv5 model:\n",
    "# model_yolo = torch.hub.load('ultralytics/yolov5', 'custom', \n",
    "#                             path='path/to/yolov5_face_weights.pt')\n",
    "\n",
    "# Then test:\n",
    "result_img, detections = detect_emotions_yolov5(test_image_path, model, model_yolo, EMOTION_CLASSES)\n",
    "if result_img is not None:\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(result_img)\n",
    "plt.title(\"Emotion Detection with YOLOv5\", fontsize=14, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print(f\"Detected {len(detections)} face(s)\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a61c3a0",
   "metadata": {},
   "source": [
    "### **10. Save and Load Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553b5f0b",
   "metadata": {},
   "source": [
    "#### 10.1. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb5a10f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
